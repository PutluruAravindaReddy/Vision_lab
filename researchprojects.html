<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="./img/srm_logo_round.png" type="image/png">
    <title>Research Projects | Computer Vision Vertical - SRMIST </title>
    <meta property="og:title" content="Research Projects | Computer Vision Vertical - SRMIST">
    <meta property="og:description" content="Computer vision Vertical website SRMIST">
    <meta property="og:image" content="./img/srm_logo_round.png" type="image/png">
    <meta property="og:url" content="https://putluruaravindareddy.github.io/Vision_lab/">
    <meta name="og:site_name" content="Vision_lab">

    <link rel="stylesheet" href="https://pro.fontawesome.com/releases/v5.10.0/css/all.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Bree+Serif&family=Lato&family=Monoton&family=Poppins:wght@400;500&family=Roboto&display=swap"
        rel="stylesheet">
    

    <link rel="stylesheet" href="./navbar.css">
    <link rel="stylesheet" href="researchprojects.css">
</head>

<body>
    <div id="bg">
        <header id="headerbg">
            <img src="./img/srm_logo.svg" alt="" class="srm_logo" height="40">
            <nav id="navbar" class="navbar">
                <ul>
                    <li> <a href="index.html">Home</a> </li>
                    <li> <a href="faculty.html">Faculty</a> </li>
                    <li> <a href="publications.html">Publications</a> </li>
                    <li class="active"> <a href="researchprojects.html">Research Projects</a> </li>
                    <li> <a href="gallery.html">Gallery</a> </li>
                    <!-- <li> <a href="cart.html"><i class="fas fa-search"></i></a> </li> -->
                </ul>
            </nav>
            <div id="sidebar" class="sidebar">
                <i class="fas fa-bars"></i>
            </div>
        </header>
        <nav id="navbar2" class="navbar2">
            <ul>
                <li> <a href="index.html">Home</a> </li>
                <li> <a href="faculty.html">Faculty</a> </li>
                <li> <a href="publications.html">Publications</a> </li>
                <li class="active"> <a href="researchprojects.html">Research Projects</a> </li>
                <li> <a href="gallery.html">Gallery</a> </li>
                <!-- <li> <a href="cart.html"><i class="fas fa-search"></i></a> </li> -->
            </ul>
        </nav>

        <section id="hero">

            <h1>VC</h1>
            <h2><span class="visual">Visual</span> Computing Vertical</h2>

        </section>
    </div>


    <section id="projects_sec">
        <h1 class=>Human detection in synergy with OSCC detection</h1>
        <div id="projects">
              
            <div class="project_div">
                <p><strong>Hardware used : </strong>Microscope and Mi 360° Camera, Oculus Quest 2</p>
                <p><strong>Software used : </strong>Python, Flask </p>
                <p><strong>Problem statement : </strong>creating a synergy of OSCC detection and Human detection using an NLP. </p>
                <p>Pose estimation is a computer vision technique that classifies the different body parts and joints of
                    a human body and tracks their movement. This is achieved by assigning different key-points or
                    ‘landmarks’ to the parts and joints of the body, and thus continuously tracking their real-time
                    location through images or video feed while providing the user with the coordinates of those
                    landmarks.
                    This project made use of Raspberry Pi, more about which will be explained later. The project was
                    made in Python coding language, and the OpenCV and MediaPipe libraries act as the most important
                    parts. The Raspberry Pi camera module was used to get live video input and various other
                    pre-recorded videos were also used to test the project out.
                </p>
            </div>
        </div>
        <div id="projects">
            <div id="slideshow">
                <div class="slideshow-container">

                    <div class="mySlides fade">
                        <div class="numbertext">1 / 3</div>
                        <img src="./img/features/computer-vision.jpeg" style="width:100%" alt="s">
                        <div class="text">Caption Text</div>
                    </div>

                    <div class="mySlides fade">
                        <div class="numbertext">2 / 3</div>
                        <img src="./img/features/image-video.jpg" style="width:100%" alt="s">
                        <div class="text">Caption Two</div>
                    </div>

                    <div class="mySlides fade">
                        <div class="numbertext">3 / 3</div>
                        <img src="./img/features/cross-platfrom.jpeg" style="width:100%" alt="s">
                        <div class="text">Caption Three</div>
                    </div>

                    <a class="prev" onclick="plusSlides(-1)">❮</a>
                    <a class="next" onclick="plusSlides(1)">❯</a>

                </div>
                <br>

                <div style="text-align:center">
                    <span class="dot" onclick="currentSlide(1)"></span>
                    <span class="dot" onclick="currentSlide(2)"></span>
                    <span class="dot" onclick="currentSlide(3)"></span>
                </div>
            </div>
            <div class="project_div">
                <p><strong>Hardware used : </strong>Microscope and Mi 360° Camera, Oculus Quest 2</p>
                <p><strong>Software used : </strong>Python, Flask </p>
                <p><strong>Problem statement : </strong>creating a synergy of OSCC detection and Human detection using an NLP. </p>
                <p>Pose estimation is a computer vision technique that classifies the different body parts and joints of
                    a human body and tracks their movement. This is achieved by assigning different key-points or
                    ‘landmarks’ to the parts and joints of the body, and thus continuously tracking their real-time
                    location through images or video feed while providing the user with the coordinates of those
                    landmarks.
                    This project made use of Raspberry Pi, more about which will be explained later. The project was
                    made in Python coding language, and the OpenCV and MediaPipe libraries act as the most important
                    parts. The Raspberry Pi camera module was used to get live video input and various other
                    pre-recorded videos were also used to test the project out.
                </p>
            </div>
        </div>
        <div id="projects">
            <div id="slideshow">
                <div class="slideshow-container">

                    <div class="mySlides fade">
                        <div class="numbertext">1 / 3</div>
                        <img src="./img/features/computer-vision.jpeg" style="width:100%" alt="s">
                        <div class="text">Caption Text</div>
                    </div>

                    <div class="mySlides fade">
                        <div class="numbertext">2 / 3</div>
                        <img src="./img/features/image-video.jpg" style="width:100%" alt="s">
                        <div class="text">Caption Two</div>
                    </div>

                    <div class="mySlides fade">
                        <div class="numbertext">3 / 3</div>
                        <img src="./img/features/cross-platfrom.jpeg" style="width:100%" alt="s">
                        <div class="text">Caption Three</div>
                    </div>

                    <a class="prev" onclick="plusSlides(-1)">❮</a>
                    <a class="next" onclick="plusSlides(1)">❯</a>

                </div>
                <br>

                <div style="text-align:center">
                    <span class="dot" onclick="currentSlide(1)"></span>
                    <span class="dot" onclick="currentSlide(2)"></span>
                    <span class="dot" onclick="currentSlide(3)"></span>
                </div>
            </div>
            <div class="project_div">
                <p><strong>Hardware used : </strong>Microscope and Mi 360° Camera, Oculus Quest 2</p>
                <p><strong>Software used : </strong>Python, Flask </p>
                <p><strong>Problem statement : </strong>creating a synergy of OSCC detection and Human detection using an NLP. </p>
                <p>Pose estimation is a computer vision technique that classifies the different body parts and joints of
                    a human body and tracks their movement. This is achieved by assigning different key-points or
                    ‘landmarks’ to the parts and joints of the body, and thus continuously tracking their real-time
                    location through images or video feed while providing the user with the coordinates of those
                    landmarks.
                    This project made use of Raspberry Pi, more about which will be explained later. The project was
                    made in Python coding language, and the OpenCV and MediaPipe libraries act as the most important
                    parts. The Raspberry Pi camera module was used to get live video input and various other
                    pre-recorded videos were also used to test the project out.
                </p>
            </div>
        </div>
    </section>

    <section id="footer" class="section-p1">
        <div id="footer-one">
            <div id="footer-image">
                <img href="index.html" src="./img/srm_logo.svg" width="120" height="60" alt="Brand_logo">
            </div>
            <h4>Contact</h4>
            <a href=""><strong>Address : </strong>SRM University ,Tech Park 15th floor</a>
            <a href=""><strong>Phone : </strong>+01 2222 365/(+91) 01 2345 6789 </a>
            <a href=""><strong>Email : </strong>example@gmail.com</a>
            <h4>Follow Us</h4>
            <div id="social-links">
                <i class="fab fa-facebook"></i>
                <i class="fab fa-twitter"></i>
                <i class="fab fa-instagram"></i>
                <i class="fab fa-youtube"></i>
            </div>
        </div>
        <div id="footer-two">
            <iframe
                src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d11999.457313859424!2d80.05174942906093!3d12.821126726594057!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x3a52f70ce1c18cd9%3A0xffb39775f24c16e9!2sTech%20Park%20Building!5e0!3m2!1sen!2sin!4v1700621687925!5m2!1sen!2sin"
                width="350" height="200" style="border:0;" allowfullscreen="" loading="lazy"
                referrerpolicy="no-referrer-when-downgrade"></iframe>
        </div>
        <div id="footer-three">
            <h4>About</h4>
            <div class="page-links">
                <a href="#">About Us</a>
                <a href="#">Faculty</a>
                <a href="#">Publications</a>
                <a href="#">Research</a>
                <a href="#">Privacy Policy</a>
                <a href="#">Terms & Conditions</a>
            </div>
        </div>

    </section>
    <div id="copy-right">
        <p>SRM ©2023 - SRM Institute of Science and Technology | All Rights Reserved</p>
    </div>


    <script src="script.js"></script>
</body>

</html>